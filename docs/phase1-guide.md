# Phase 1: パーセプトロン学習ガイド

## 🎯 学習目標

Phase 1では、ニューラルネットワークの最も基本的な構成要素であるパーセプトロンを段階的に理解し、実装します。

### 数学的基礎

パーセプトロンは以下の数式で表されます：

```
y = σ(w·x + b)
```

ここで：
- `x` = 入力ベクトル
- `w` = 重みベクトル  
- `b` = バイアス項
- `σ` = 活性化関数（ステップ関数）
- `y` = 出力（0 または 1）

### 学習規則

パーセプトロン学習規則：

```
Δw = α(t - y)x
Δb = α(t - y)
```

ここで：
- `α` = 学習率
- `t` = 目標出力
- `y` = 実際の出力
- `Δw` = 重みの更新量
- `Δb` = バイアスの更新量

## 🛠 実装の特徴

### 学習重視設計

1. **明示的実装**: 各計算ステップを明確に分離
2. **段階的理解**: 複雑さを避けて基本原理に集中
3. **数値検証**: 既知解との比較による正確性確認

### パフォーマンス

ベンチマーク結果（Apple M4 Pro）：
- **Forward**: 43.53 ns/op （前向き計算）
- **Training**: 15.26 μs/op （1000サンプル訓練）
- **Accuracy**: 1.86 μs/op （1000サンプル精度計算）

## 📊 実験と学習効果

### 1. 線形分離可能問題

**AND ゲート**
```bash
./bin/bee train -data datasets/and.csv -output models/and.json -verbose
```

期待結果：
- 収束エポック: 5-10回
- 精度: 100%
- 学習効果: 線形分離の理解

**OR ゲート**
```bash
./bin/bee train -data datasets/or.csv -output models/or.json -verbose
```

期待結果：
- 収束エポック: 2-5回
- 精度: 100%
- 学習効果: 学習速度の違い理解

### 2. 非線形分離不可能問題

**XOR ゲート**
```bash
./bin/bee train -data datasets/xor.csv -output models/xor.json -verbose
```

期待結果：
- 収束エポック: 1000回（非収束）
- 精度: ≤ 75%
- 学習効果: 単一パーセプトロンの限界理解

## 🧪 テストによる学習効果確認

### 基本機能テスト
```bash
cd phase1
go test -v -run TestPerceptronBasicFunctionality
```

学習ポイント：
- 構造体初期化パターン
- エラーハンドリング
- 入力検証

### 学習能力テスト
```bash
go test -v -run TestPerceptronLearning
```

学習ポイント：
- 収束挙動
- 重み更新メカニズム
- 線形分離性

### XOR問題テスト
```bash
go test -v -run TestXORProblem
```

学習ポイント：
- 単一パーセプトロンの限界
- 非線形問題の認識
- 次フェーズ（MLP）への動機

### 数値精度テスト
```bash
go test -v -run TestNumericalPrecision
```

学習ポイント：
- 浮動小数点演算の課題
- 学習率の影響
- 数値安定性

## 🎓 段階的学習プロセス

### ステップ 1: 基本理解
1. パーセプトロンの数学的モデル理解
2. 重み・バイアスの役割理解
3. ステップ関数の動作理解

### ステップ 2: 実装詳細
1. `phase1/perceptron.go`のコード読解
2. 各メソッドの役割理解
3. 学習アルゴリズムの実装詳細

### ステップ 3: 実験と検証
1. AND/ORゲートでの学習確認
2. XORでの限界確認
3. パラメータ調整実験

### ステップ 4: 性能分析
1. ベンチマーク実行と分析
2. メモリ使用量確認
3. 最適化ポイント特定

## 🔍 重要な学習ポイント

### 1. 線形分離性
- 単一パーセプトロンは線形分離可能な問題のみ解決可能
- XORは線形分離不可能な典型例
- この限界がMLPの必要性を示す

### 2. 学習率の影響
```bash
# 異なる学習率での実験
./bin/bee train -data datasets/and.csv -lr 0.01 -verbose
./bin/bee train -data datasets/and.csv -lr 0.5 -verbose
./bin/bee train -data datasets/and.csv -lr 1.0 -verbose
```

観察ポイント：
- 収束速度の違い
- 重みの更新パターン
- 数値安定性

### 3. 初期化の影響
- Xavier初期化の効果
- ランダムシードによる再現性
- 初期値が学習に与える影響

## 🚀 次のステップ

Phase 1完了後の発展：

1. **Phase 1.1: MLP実装**
   - 多層構造の理解
   - 誤差逆伝播の実装
   - XOR問題の解決

2. **最適化改善**
   - 異なる活性化関数の実験
   - 学習率スケジューリング
   - ミニバッチ学習

3. **可視化追加**
   - 決定境界の可視化
   - 重みの変化追跡
   - 学習曲線の描画

## 📚 参考資料

### 理論的背景
- McCulloch-Pitts neuron model (1943)
- Rosenblatt's Perceptron (1957)
- Minsky & Papert's limitations analysis (1969)

### 実装パターン
- エラーハンドリングベストプラクティス
- 数値計算の安定性
- テスト駆動開発

### 性能最適化
- Go言語での数値計算最適化
- メモリ効率的なデータ構造
- 並行処理への拡張可能性

---

**次回予告**: Phase 1.1では、単一パーセプトロンの限界を突破する多層パーセプトロン（MLP）と誤差逆伝播を実装し、XOR問題を解決します！